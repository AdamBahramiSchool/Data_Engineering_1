# Data_Engineering_1

## Kafka, Spark, Airflow, Cassandra, Postgres

### Overview
This project demonstrates a data engineering pipeline using various technologies for orchestration, data streaming, processing, and storage. The pipeline retrieves random user data from an API, processes it, and stores it in databases using distributed systems and real-time streaming.

### Components

1. **Data Source: randomuser.me API**
   - This API generates random user data, which serves as the initial data source for the pipeline.

2. **Apache Airflow**
   - **Orchestration**: Apache Airflow manages the workflow of the data pipeline, ensuring that tasks are executed in a specific order and handling task dependencies.
   - **Data Storage**: Airflow fetches data from the randomuser.me API and stores it in a PostgreSQL database.

3. **PostgreSQL**
   - **Database**: PostgreSQL is used to store the raw data fetched by Apache Airflow from the API.

4. **Apache Kafka and Zookeeper**
   - **Data Streaming**: Apache Kafka streams data from the PostgreSQL database to the processing engine.
   - **Distributed Synchronization**: Zookeeper works alongside Kafka to manage and coordinate the Kafka brokers, ensuring distributed synchronization.

5. **Control Center and Schema Registry**
   - **Monitoring**: The Control Center helps monitor the health and performance of Kafka streams.
   - **Schema Management**: The Schema Registry manages the versioning of schemas used by Kafka topics, ensuring data consistency.

6. **Apache Spark**
   - **Data Processing**: Apache Spark processes the streamed data. It utilizes a master-worker architecture, where the master node coordinates work across multiple worker nodes for distributed data processing.

7. **Cassandra**
   - **Data Storage**: The processed data from Apache Spark is stored in Cassandra, a NoSQL database known for its scalability and high availability.

8. **Docker**
   - **Containerization**: Docker is used to containerize all the components of the system architecture, making it easier to deploy and manage the entire data engineering setup.

### What Youâ€™ll Learn
- Setting up a data pipeline with Apache Airflow
- Real-time data streaming with Apache Kafka
- Distributed synchronization with Apache Zookeeper
- Data processing techniques with Apache Spark
- Data storage solutions with Cassandra and PostgreSQL
- Containerizing your entire data engineering setup with Docker

### Technologies Used
- Apache Airflow
- Python
- Apache Kafka
- Apache Zookeeper
- Apache Spark
- Cassandra
- PostgreSQL
- Docker
